\section{Related Work}


The PCAST OpenACC autocompare feature is similar in some respects to redundant execution strategies, which are typically used to detect failing hardware or erroneous software.
The Tandem NonStop computers~\cite{bartlett.tandem.86} implemented redundant execution on identical hardware with automatic checking; the system could detect faulty hardware and fail-over to another processor.
The NASA Space Shuttle carried five computers~\cite{fraser.astro.74}; four of these comprised the primary system and ran identical software with a voting protocol to detect a failing computer.
If the four primary system computers could not determine a correct result, the fifth backup system was enabled for ascent and landing.
The major difference is the autocompare feature assumes that the CPU execution is correct, and it compares the GPU computations to those assumed correct results.

The Cray Comparative Debugger (CCDB)~\cite{derose.sc.15} which is based on the relative debugging paradigm proposed by Abramson et al. \cite{abramson1994relative}\cite{abramson1996relative} allows a programmer to launch two versions of a program, such as a CPU-only version and a GPU-accelerated version, and to inspect and compare values between the two versions.
The programmer can also add breakpoints and have the debugger compare specific values between the two program versions when each reaches the breakpoint.
This is perhaps the most aggressive approach to allow value comparisons between two running executions and allows a user to inspect the program when the values diverge, although the comparisons themselves are not performed automatically.

Research using the OpenARC compiler framework~\cite{lee.hpdc.14} has explored several strategies for debugging OpenACC programs~\cite{lee.ipdps.14}.
One mechanism is very much like the PCAST autocompare feature, where the compiler generates device code and host sequential code for specific compute regions.
The user selects specific compute regions to verify, and the rest of the program is executed sequentially by the host CPU, including other compute regions.
All the data is copied from the system memory to the GPU before those selected kernels, and all modified data is copied back and compared afterward.
That work allows more fine grain control of which compute regions to compare, but does not allow for a unified framework to run the whole program and compare data after each compute region.

\section{Future Work}

We are considering future work on the autocompare feature, including:
\begin{itemize}
\item Optimizing the comparison operation, since this seems to be the bottleneck for the autocompare feature.
\item Ways to reduce the number of values being compared, to reduce the runtime cost.
\item Running the compare itself in parallel, and perhaps running the compare code on the GPU itself.
\item Some way to isolate regions of the program that are being tested, so as to not require redundant execution and data compares throughout the whole execution.
\item Running the host code in parallel as well, for performance, though this assumes that host parallel execution is as accurate and precise as the host sequential execution.
\item Adding support for arrays of structs or arrays of derived types, where each field would have a type-specific compare.
\item Adding support for nested data structures, where a struct memory is a pointer to another array.
The compare function would recurse to compare the nested structure as well.
\item Similar to the idea of record and replay \cite{sato2015clock} we might add support to allow comparisons across runs of the same autocompare and observe if there are any differences between different runs. This reveals if the bug is reproducible or not, and could potentially help determine whether the problem is more than just a round off error. 
\item Specifying an arbitrary numerical tolerance through the relative or the absolute tolerance environment variable options may result in missed bugs or spurious false bugs reported, especially when the values are very small. Furthermore, some applications require bitwise equivalent which is a strong requirement and might not be possible especially in non-deterministic applications. A solution would be adding a third environment variable option to the tolerance options suite which would take into account the magnitude of the values and perform the comparisons in a normalized fashion. 
\end{itemize}

\section{Conclusion}

While directive based GPU programming models improve the productivity of GPU programming, they do not ensure correctness or the absence of logical errors.
These programming models usually consist of several compiler optimizations and transformations that are hidden from the programmer.
These models improve productivity but unfortunately present challenges to users who must debug and verify program correctness.
This work, part of the PCAST feature, can automatically detect numerical differences that can occur due to computational differences on different OpenACC devices.
Hence, PCAST can automatically check the correctness of an OpenACC implementation and it can also help to identify bugs in OpenACC data management and computation and support programmers in the development of OpenACC applications.

The overhead incurred due to redundant execution is dominated by the slower execution unit and is relatively small, while the overhead of the compare operation is significant.
However, automatic debuggers and correctness checkers always introduce some overhead, and in most cases the total cost is much less than manual investigation.
