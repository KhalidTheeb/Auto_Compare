\section{Related Work}


The PCAST OpenACC autocompare feature is similar in some respects to redundant execution strategies, which are typically used to detect failing hardware or erroneous software.
The Tandem NonStop computers~\cite{bartlett.tandem.86} implemented redundant execution on identical hardware with automatic checking; the system could detect faulty hardware and fail-over to another processor.
The NASA Space Shuttle carried five computers~\cite{fraser.astro.74}; four of these comprised the primary system and ran identical software with a voting protocol to detect a failing computer.
If the four primary system computers could not determine a correct result, the fifth backup system was enabled for ascent and landing.
The major difference is the autocompare feature assumes that the CPU execution is correct, and it compares the GPU computations to those assumed correct results.

The Cray Comparative Debugger (CCDB)~\cite{derose.sc.15} allows a programmer to launch two versions of a program, such as a CPU-only version and a GPU-accelerated version, and to inspect and compare values between the two versions.
The programmer can also add breakpoints and have the debugger compare specific values between the two program versions when the two versions reach the breakpoint.
This is perhaps the most aggressive approach to allow value comparisons between two running executions and allows a user to inspect the program when the values diverge, although the comparisons themselves are not performed automatically.

Research using the OpenARC compiler framework~\cite{lee.hpdc.14} has explored several strategies for debugging OpenACC programs~\cite{lee.ipdps.14}.
One of these strategies is a mechanism very like the PCAST autocompare feature, where the compiler generates device code and host sequential code for specific compute regions.
The user selects specific compute regions to verify, and the rest of the program is executed sequentially by the host CPU, including other compute regions.
All the data is copied from the system memory to the GPU before those selected kernels, and all modified data is copied back and compared afterward.
That work allows more fine grain control of which compute regions to compare, but does not allow for a unified framework to run the whole program and compare data after each compute region.

\section{Future Work}

We are considering future work on the autocompare feature, including:
\begin{itemize}
\item Optimizing the comparison operation, since this seems to be the bottleneck for the autocompare feature.
\item Ways to reduce the number of values being compared, to reduce the runtime cost.
\item Running the compare itself in parallel, and perhaps running the compare code on the GPU itself.
\item Some way to isolate regions of the program that are being tested, so as to not require redundant execution and data compares throughout the whole execution.
\item Running the host code in parallel as well, for performance, though this assumes that host parallel execution is as accurate and precise as the sequential execution.
\item Adding support for arrays of structs or arrays of derived types, where each field would have a type-specific compare.
\item Adding support for nested data structures, where a struct memory is a pointer to another array.
The compare function would recurse to compare the nested structure as well.
\end{itemize}
